<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>初识 Hadoop</title>
      <link href="/2018/11/01/%E5%88%9D%E8%AF%86%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
      <url>/2018/11/01/%E5%88%9D%E8%AF%86%E5%A4%A7%E6%95%B0%E6%8D%AE/</url>
      
        <content type="html"><![CDATA[<p>大数据生态圈，主要包含 Hadoop 生态圈和 Spark 生态圈。Hadoop 主要包含分布式文件系统 HDFS，分布式资源调度平台 YARN，分布式计算框架 MapReduce。大数据主要设计到的技术：数据采集，数据处理/分析/挖掘，数据存储，可视化。</p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p><a href="http://hadoop.apache.org/" target="_blank" rel="noopener">Hadoop 官网</a><br>Hadoop 是开源的分布式存储 + 分布式计算平台；主要用于日志分析、基于海量数据的离线应用、推荐系统、计算广告、复杂算法、网盘、搜索引擎等。</p><ul><li><p>高可靠性</p><p>数据存储：数据块多副本；数据计算：重新调度作业计算。</p></li><li><p>高扩展性</p><p>存储/计算资源不够时，可以横向的线性扩展机器；一个集群中可以包含数以千计的节点；可以存储在廉价的机器上，降低成本；并且有成熟的生态圈。</p></li></ul><p><img src="/images/hadoop/简介.png" alt="Hadoop 概述"></p><p><img src="/images/hadoop/EA8BABFE-9845-44FE-BE6B-381ECF995EC2.png" alt="Hadoop 生态圈"></p><h3 id="主要模块"><a href="#主要模块" class="headerlink" title="主要模块"></a>主要模块</h3><ul><li><p>Hadoop Common</p><p>为其它Hadoop模块提供基础设施</p></li><li><p>Hadoop HDFS</p><p>一个高可靠、高吞吐量的分布式文件系统</p></li><li><p>Hadoop MapReduce</p><p> 一个分布式的离线并行计算框架</p></li><li><p>Hadoop YARN</p><p> Hadoop 2.x 的 MapReduce 框架，任务调度与资源管理</p></li></ul><h3 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h3><p>分布式文件系统，将文件切分成指定大小的数据快并以多副本的方式存储在多个机器上，数据切分，多副本，容错等操作对用户是透明的。</p><ul><li>源自于 Google 的 GFS 论文</li><li>HDFS 是 GFS 的克隆版</li><li>特点：扩展性 &amp; 容错性 &amp; 海量数据存储</li></ul><p><img src="/images/hadoop/0D0E06B8-3E6C-422B-AD10-4CF21C8F73A8.png" alt="HDFS 文件存储"></p><h3 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h3><p>资源调度系统。</p><ul><li>YARN：Yet Another Resource Negotiator</li><li>负责整个其群资源的管理和调度</li><li>特点：扩展性 &amp; 容错性 &amp; 多框架资源统一调度</li></ul><p><img src="/images/hadoop/851C8A93-83EE-4483-881C-87A7D1DC0B41.png" alt="YARN 资源调度系统"></p><h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>分布式计算框架。</p><ul><li>源自 Google 的 MapReduce 论文</li><li>是 Google MapReduce 的克隆版</li><li>特点：扩展性 &amp; 容错性 &amp; 海量数据离线处理</li><li>缺点：实时流式计算</li></ul><p><img src="/images/hadoop/56CA8D65-1D5A-4332-9812-18C1D65EC084.png" alt="MapReduce 处理过程"></p><h3 id="常用发型版本"><a href="#常用发型版本" class="headerlink" title="常用发型版本"></a>常用发型版本</h3><ul><li><a href="http://hadoop.apache.org/" target="_blank" rel="noopener">Apache Hadoop</a></li><li><a href="https://www.cloudera.com/downloads/cdh/6-0-1.html" target="_blank" rel="noopener">CDH Cloudera Distributed Hadoop</a></li><li><a href="https://hortonworks.com/products/data-platforms/hdp/" target="_blank" rel="noopener">HDP Hortonworks Data Platform</a></li></ul><h2 id="HDFS-1"><a href="#HDFS-1" class="headerlink" title="HDFS"></a>HDFS</h2><h3 id="HDFS-架构"><a href="#HDFS-架构" class="headerlink" title="HDFS 架构"></a>HDFS 架构</h3><p>HDFS 优点：数据冗余、硬件容错；处理流式的数据访问；适合存储大文件；可以构建在廉价的机器上。<br>HDFS 缺点：低延迟的数据访问；不适合小文件存储。</p><ul><li>1个Master（NameNode/NN）带N个Slaves（DataNode/DN），一般部署在不同的节点上</li><li>1个文件会被拆分成多个 Block，默认大小为 128M</li><li>NN：负责客户端请求的响应；负责元数据（文件的名称，副本系数，Block存放DN）的管理</li><li>DN：负责用户的文件对应的数据块（Block），定期向 NN 发送心跳信息，汇报本身及其所有 Block 信息，健康状况</li></ul><h3 id="HDFS-Commands"><a href="#HDFS-Commands" class="headerlink" title="HDFS Commands"></a>HDFS Commands</h3><p>参考官网 <a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html" target="_blank" rel="noopener">HDFS Commands</a></p><p>为方便操作，将 $HADOOP_HOME/bin 配置到环境变量 PATH 路径中。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop-2.6.0-cdh5.7.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure></p><h4 id="dfs"><a href="#dfs" class="headerlink" title="dfs"></a>dfs</h4><p>查看 dfs 操作所有命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs <span class="comment"># 等价于 hadoop fs</span></span><br></pre></td></tr></table></figure></p><h4 id="ls"><a href="#ls" class="headerlink" title="ls"></a>ls</h4><p>列出对应 dfs 目录下的文件夹和文件，-R，递归展示<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -ls /</span><br><span class="line">hdfs dfs -ls -R / <span class="comment"># 递归展示</span></span><br></pre></td></tr></table></figure></p><h4 id="put"><a href="#put" class="headerlink" title="put"></a>put</h4><p>将本地文件上传至 dfs 指定目录<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim hello.txt</span><br><span class="line">hdfs dfs -put hello.txt /</span><br></pre></td></tr></table></figure></p><h4 id="text-cat"><a href="#text-cat" class="headerlink" title="text/cat"></a>text/cat</h4><p>查看 dfs 文件内容<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -text /hello.txt</span><br><span class="line">hdfs dfs -cat /hello.txt</span><br></pre></td></tr></table></figure></p><h4 id="mkdir"><a href="#mkdir" class="headerlink" title="mkdir"></a>mkdir</h4><p>在 dfs 上创建文件夹<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir /<span class="built_in">test</span></span><br><span class="line">hdfs dfs -mkdir -p /<span class="built_in">test</span>/a/b <span class="comment"># 递归创建</span></span><br></pre></td></tr></table></figure></p><h4 id="copyFromLocal"><a href="#copyFromLocal" class="headerlink" title="copyFromLocal"></a>copyFromLocal</h4><p>将本地文件 copy 到 dfs<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -copyFromLocal hello.txt /<span class="built_in">test</span>/a/b/h.txt</span><br></pre></td></tr></table></figure></p><h4 id="get"><a href="#get" class="headerlink" title="get"></a>get</h4><p>将 dfs 文件 copy 到本地<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -get /<span class="built_in">test</span>/a/b/h.txt</span><br></pre></td></tr></table></figure></p><h4 id="rm"><a href="#rm" class="headerlink" title="rm"></a>rm</h4><p>删除文件和文件夹，-R，递归删除<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -rm /hello.txt</span><br><span class="line">hdfs dfs -rm -R /<span class="built_in">test</span></span><br></pre></td></tr></table></figure></p><h3 id="JAVA-操作-HDFS"><a href="#JAVA-操作-HDFS" class="headerlink" title="JAVA 操作 HDFS"></a>JAVA 操作 HDFS</h3><h4 id="引入依赖"><a href="#引入依赖" class="headerlink" title="引入依赖"></a>引入依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>2.6.0-cdh5.7.0<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="操作-API"><a href="#操作-API" class="headerlink" title="操作 API"></a>操作 API</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Progressable;</span><br><span class="line"><span class="keyword">import</span> org.junit.After;</span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * HDFS 操作</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> zixi.xie</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2018/11/1</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HDFSApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 需要配置将对应服务器添加到开发机器上的 hosts 上</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String HDFS_PATH = <span class="string">"hdfs://hadoop-node:9000"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> FileSystem fileSystem = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Configuration configuration = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUp</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"HDFSApp.setUp"</span>);</span><br><span class="line">        configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">// 解决使用内网部署集群问题</span></span><br><span class="line">        configuration.set(<span class="string">"dfs.client.use.datanode.hostname"</span>, <span class="string">"true"</span>);</span><br><span class="line">        fileSystem = FileSystem.get(<span class="keyword">new</span> URI(HDFS_PATH), configuration, <span class="string">"root"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">tearDown</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        fileSystem.close();</span><br><span class="line">        configuration = <span class="keyword">null</span>;</span><br><span class="line">        System.out.println(<span class="string">"HDFSApp.tearDown"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建目录</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mkdir</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/test"</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建文件</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 创建文件 指定副本数 进度条</span></span><br><span class="line">        FSDataOutputStream outputStream = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"/hello .txt"</span>), (<span class="keyword">short</span>) <span class="number">1</span>, <span class="keyword">new</span> Progressable() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">progress</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                System.out.print(<span class="string">"."</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        outputStream.write(<span class="string">"Hello World"</span>.getBytes());</span><br><span class="line">        outputStream.flush();</span><br><span class="line">        IOUtils.closeStream(outputStream);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 查看文件内容</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cat</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        FSDataInputStream inputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/hadoop.txt"</span>));</span><br><span class="line">        System.out.println(inputStream);</span><br><span class="line">        IOUtils.copyBytes(inputStream, System.out, <span class="number">1024</span>);</span><br><span class="line">        IOUtils.closeStream(inputStream);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 重命名</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">rename</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        Path oldPath = <span class="keyword">new</span> Path(<span class="string">"/hadoop.txt"</span>);</span><br><span class="line">        Path newPath = <span class="keyword">new</span> Path(<span class="string">"/hadoop-new.txt"</span>);</span><br><span class="line">        fileSystem.rename(oldPath, newPath);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 将本地文件拷贝到 HDFS</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">copyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        Path localPath = <span class="keyword">new</span> Path(<span class="string">"/Users/xiezixi/Project/study/study-hadoop/pom.xml"</span>);</span><br><span class="line">        Path hdfsPath = <span class="keyword">new</span> Path(<span class="string">"/test"</span>);</span><br><span class="line">        fileSystem.copyFromLocalFile(localPath, hdfsPath);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 下载 HDFS 文件到本地</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">copyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        Path localPath = <span class="keyword">new</span> Path(<span class="string">"/Users/xiezixi/Project/study/study-hadoop/tmp/hadoop.txt"</span>);</span><br><span class="line">        Path hdfsPath = <span class="keyword">new</span> Path(<span class="string">"/hello.txt"</span>);</span><br><span class="line">        fileSystem.copyToLocalFile(hdfsPath, localPath);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 列出 HDFS 上面的文件和文件夹</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listFiles</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        FileStatus[] fileStatuses = fileSystem.listStatus(<span class="keyword">new</span> Path(<span class="string">"/"</span>));</span><br><span class="line">        <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">            String isDir = fileStatus.isDirectory() ? <span class="string">"文件夹"</span> : <span class="string">"文件"</span>;</span><br><span class="line">            <span class="keyword">short</span> replication = fileStatus.getReplication();</span><br><span class="line">            <span class="keyword">long</span> len = fileStatus.getLen();</span><br><span class="line">            String path = fileStatus.getPath().toString();</span><br><span class="line"></span><br><span class="line">            System.out.println(isDir + <span class="string">"\t"</span> + replication + <span class="string">"\t"</span> + len + <span class="string">"\t"</span> + path);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除 HDFS 文件</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        fileSystem.delete(<span class="keyword">new</span> Path(<span class="string">"/hello.txt"</span>), <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="报错解决"><a href="#报错解决" class="headerlink" title="报错解决"></a>报错解决</h4><ul><li><p>host 连接不上<br><pre>java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: “xiezixideMacBook-Pro.local/10.2.114.114”; destination host is: “hadoop-node”:9000;</pre><br><img src="/images/hadoop/B74D4E71-C72C-48BD-9F18-9E27A501B70E.png" alt="报错1"><br>解决方法： 部署时，需要将 core-site.xml，slaves 设置为 hostname，<code>&lt;value&gt;hdfs://hadoop-node:9000&lt;/value&gt;</code>。</p></li><li><p>用户权限错误<br><pre>org.apache.hadoop.security.AccessControlException: Permission denied: user=xiezixi, access=WRITE, inode=”/“:root:supergroup:drwxr-xr-x</pre><br><img src="/images/hadoop/42F185F8-6C38-48CD-BAF4-06B9CABAD399.png" alt="报错2"><br>解决方法： 创建 fileSystem 对象时，指定用户，<code>fileSystem = FileSystem.get(new URI(HDFS_PATH), configuration, &quot;root&quot;);</code>。</p></li></ul><h2 id="YARN-1"><a href="#YARN-1" class="headerlink" title="YARN"></a>YARN</h2><h3 id="YARN-架构"><a href="#YARN-架构" class="headerlink" title="YARN 架构"></a>YARN 架构</h3><h4 id="Hadoop-1-x"><a href="#Hadoop-1-x" class="headerlink" title="Hadoop 1.x"></a>Hadoop 1.x</h4><p>Hadoop 1.x 时，MapReduce 1.x 存在问题：单节点故障 &amp; 节点压力大不易扩展。</p><ul><li>MapReduce：Master/Slave 架构，1个 JobTracker 带多个 TaskTracker</li><li>JobTracker：负责资源管理和作业调度</li><li>TaskTracker：定期向 JT 汇报本节点的健康状况，资源使用情况，作业执行情况，结构来自 JT的命令：启动任务/杀死任务</li></ul><p><img src="/images/hadoop/A843CA90-A276-41D1-B180-6CA6DAE65E3F.png" alt="MapReduce 1.x 问题"></p><p>YARN：不同计算框架可以共享在同一个 HDFS 集群上的数据，享受整体的资源调度。</p><p><img src="/images/hadoop/843C7082-8087-4455-A24C-1EFFD5314910.png" alt="YARN 架构"></p><h4 id="Hadoop-2-x"><a href="#Hadoop-2-x" class="headerlink" title="Hadoop 2.x"></a>Hadoop 2.x</h4><ul><li><p>ResourceManager</p><p>整个集群同一时间提供服务的 RM 只有一个，负责集群资源的统一管理和调度；处理客户端的请求：提交一个作业、杀死一个作业；监控 NM，一旦某个 NM 挂了，那么该 NM 上运行的任务需要告诉 AM 该如何进行处理；</p></li><li><p>NodeManager</p><p>整个集群中有多个，负责自己本身节点资源管理和使用；定时向 RM 汇报本节点的资源使用情况；接收并处理来自 RM 的各种命令，启动 Container 等；处理来自 AM 的命令；单个节点的资源管理；</p></li><li><p>ApplicationMaster</p><p>每个应用程序对应一个：MR，Spark等，负责应用程序的管理；为应用程序向 RM 申请资源（CPU、memory），分配给内部的 task；需要与 NM 通信：启动/停止 task，task 是运行在 Container 里面的，AM 也是运行在 Container 里面；</p></li><li><p>ApplicationMaster</p><p>封装了 CPU、Memory 等资源的一个容器；是一个任务运行环境的抽象；</p></li><li><p>Client</p><p>提交作业；查询作业的运行进度；杀死作业；</p></li></ul><h2 id="MapReduce-1"><a href="#MapReduce-1" class="headerlink" title="MapReduce"></a>MapReduce</h2><h3 id="MapReduce-编程模型"><a href="#MapReduce-编程模型" class="headerlink" title="MapReduce 编程模型"></a>MapReduce 编程模型</h3><ul><li>将作业拆分为 Map 阶段和 Reduce 阶段；</li><li>Map 阶段：Map Tasks</li><li>Reduce 阶段：Reduce Task</li></ul><h4 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h4><blockquote><p>(input) &lt;k1, v1&gt; -&gt; map -&gt; &lt;k2, v2&gt; -&gt; combine -&gt; &lt;k2, v2&gt; -&gt; reduce -&gt; &lt;k3, v3&gt; (output)</p></blockquote><ul><li><p>Split</p><p>交由 MapReduce 作业处理的数据块，是 MapReduce 中最小的计算单元；HDFS 中，blockSize 是最小的存储单元，默认为 128M。默认情况下：它们是一一对应的；</p></li></ul><ul><li><p>InputFormat</p><p>将输入的数据进行分片（Split），如 TextInputFormat 用于处理文本格式的数据；</p></li><li><p>OutputFormat  </p><p>输出</p></li><li><p>Combiner</p><p>本地的 Reducer，能减少 Map Task 输出的数据量及网络传输量；</p></li><li><p>Partitioner</p><p>决定 Map Task 输出的数据交由哪个 Reduce Task 处理；默认实现：分发的 key 的 hash 值对 Reduce Task 个数取模；</p></li></ul><h4 id="WordCount-编程模型"><a href="#WordCount-编程模型" class="headerlink" title="WordCount 编程模型"></a>WordCount 编程模型</h4><ol><li>准备 Map 处理的输入数据</li><li>Map 处理</li><li>Shuffle</li><li>Reduce 处理</li><li>结果输出</li></ol><p><img src="/images/hadoop/7E4A8B45-9D07-4209-9266-22B6B87361F9.png" alt="WordCount编程模型"></p><h3 id="MapReduce-架构"><a href="#MapReduce-架构" class="headerlink" title="MapReduce 架构"></a>MapReduce 架构</h3><h4 id="Hadoop-1-x-1"><a href="#Hadoop-1-x-1" class="headerlink" title="Hadoop 1.x"></a>Hadoop 1.x</h4><ul><li><p>JobTracker</p><p>作业的管理者，将作业分解为一堆任务：Task（MapTask 和 ReduceTask）；将任务分配给 TaskTracker 运行；作业的监控、容错处理（作业挂了，重启 task 机制）；</p></li><li><p>TaskTracker</p><p>任务的执行者，执行任务，与 JT 进行交互：执行/启动/停止作业；发送心跳信息给 JT；</p></li><li><p>MapTask</p><p>Map 任务交由 task 处理，解析每条记录的数据，交给自己的 Map 方法处理；将 Map 的输出结果写到本地磁盘（由的作业仅只有 Map，没有 Reduce）</p></li><li><p>ReduceTask</p><p>将 MapTask 输出的数据进行读取，安装数据进行分组传给自己的 Reduce 方法处理，输出结果；</p></li></ul><h4 id="Hadoop-2-x-1"><a href="#Hadoop-2-x-1" class="headerlink" title="Hadoop 2.x"></a>Hadoop 2.x</h4><p><img src="/images/hadoop/49011E3C-D074-4425-AB05-D007C250CDAD.png" alt="Hadoop 2.x"></p><h3 id="WordCount-开发示例"><a href="#WordCount-开发示例" class="headerlink" title="WordCount 开发示例"></a>WordCount 开发示例</h3><h4 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mapreduce;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用 MapReduce 开发 WordCount</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> zixi.xie</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2018/11/3</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Map: 输出输入文件</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        LongWritable one = <span class="keyword">new</span> LongWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 接收到的每一行数据</span></span><br><span class="line">            String line = value.toString();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 按照指定分隔符进行拆份</span></span><br><span class="line">            String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                <span class="comment">// 通过上下文把 map 的处理结果输出</span></span><br><span class="line">                context.write(<span class="keyword">new</span> Text(word), one);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 归并操作</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;LongWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">long</span> sum = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (LongWritable value : values) &#123;</span><br><span class="line">                <span class="comment">// key 出现次数的总和</span></span><br><span class="line">                sum += value.get();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 最终统计解决输出</span></span><br><span class="line">            context.write(key, <span class="keyword">new</span> LongWritable(sum));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建 Configuration</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 清理已存在的输出目录</span></span><br><span class="line">        Path outputPath = <span class="keyword">new</span> Path(args[<span class="number">1</span>]);</span><br><span class="line">        FileSystem fileSystem = FileSystem.get(configuration);</span><br><span class="line">        <span class="keyword">if</span> (fileSystem.exists(outputPath)) &#123;</span><br><span class="line">            fileSystem.delete(outputPath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建 Job</span></span><br><span class="line">        Job job = Job.getInstance(configuration, <span class="string">"wordCount"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 Job 的处理类</span></span><br><span class="line">        job.setJarByClass(WordCountApp.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置作业处理输入路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path[]&#123;<span class="keyword">new</span> Path(args[<span class="number">0</span>])&#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 Map 相关参数</span></span><br><span class="line">        job.setMapperClass(MyMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(LongWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 Reduce 相关参数</span></span><br><span class="line">        job.setReducerClass(MyReducer.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(LongWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置作业处理的输出路径</span></span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="打包上传至服务器运行"><a href="#打包上传至服务器运行" class="headerlink" title="打包上传至服务器运行"></a>打包上传至服务器运行</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本地</span></span><br><span class="line">mvn clean package -DskipTests</span><br><span class="line"><span class="built_in">cd</span> target/</span><br><span class="line">scp study-hadoop-1.0-SNAPSHOT.jar root@35.243.201.189:/tmp</span><br><span class="line"><span class="comment"># 远程</span></span><br><span class="line">hadoop jar study-hadoop-1.0-SNAPSHOT.jar mapreduce.WordCountApp hdfs://hadoop-node:9000/hello.txt hdfs://hadoop-node:9000/output/wc</span><br></pre></td></tr></table></figure><h2 id="部署搭建"><a href="#部署搭建" class="headerlink" title="部署搭建"></a>部署搭建</h2><p>操作系统为 CentOS 7.3。</p><h3 id="伪分布式安装"><a href="#伪分布式安装" class="headerlink" title="伪分布式安装"></a>伪分布式安装</h3><h4 id="修改-host"><a href="#修改-host" class="headerlink" title="修改 host"></a>修改 host</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts <span class="comment"># 修改 hosts 后重启</span></span><br><span class="line">reboot</span><br></pre></td></tr></table></figure><h4 id="安装-JDK"><a href="#安装-JDK" class="headerlink" title="安装 JDK"></a>安装 JDK</h4><p>到 <a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">oracle 官网</a> 复制 JDK 下载地址。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">wget https://download.oracle.com/otn-pub/java/jdk/8u191-b12/2787e4a523244c269598db4e85c51e0c/jdk-8u191-linux-x64.tar.gz?AuthParam=1541057549_a8ee4101538c337d3b0c87d572832f2c</span><br><span class="line">tar -zxvf jdk-8u191-linux-x64.tar.gz\?AuthParam\=1541057549_a8ee4101538c337d3b0c87d572832f2c -C /usr/<span class="built_in">local</span>/</span><br><span class="line">vim /etc/profile</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_191</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib/tools.jar</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure></p><h4 id="安装-SSH"><a href="#安装-SSH" class="headerlink" title="安装 SSH"></a>安装 SSH</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yum install -y openssh-server</span><br><span class="line">service sshd start</span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line">cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys</span><br><span class="line">ssh localhost <span class="comment"># 验证是否配置成功</span></span><br></pre></td></tr></table></figure><h4 id="安装-Hadoop"><a href="#安装-Hadoop" class="headerlink" title="安装 Hadoop"></a>安装 Hadoop</h4><p><strong>安装 hadoop-2.6.0-cdh5.7.0</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget http://archive-primary.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.0.tar.gz</span><br><span class="line">tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz -C /usr/<span class="built_in">local</span>/</span><br><span class="line">vim /etc/profile</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop-2.6.0-cdh5.7.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure></p><p><strong>修改 HDFS 配置</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/hadoop-2.6.0-cdh5.7.0/etc/hadoop</span><br><span class="line">vim hadoop-env.sh <span class="comment"># 配置 JAVA_HOME</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_191</span><br><span class="line">vim core-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop-node:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">vim hdfs-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">vim slaves</span><br><span class="line">hadoop-node</span><br></pre></td></tr></table></figure></p><p><strong>格式化 HDFS 文件系统</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/hadoop-2.6.0-cdh5.7.0</span><br><span class="line">bin/hdfs namenode -format <span class="comment"># 格式化文件系统</span></span><br></pre></td></tr></table></figure></p><p><strong>启动 HDFS</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/hadoop-2.6.0-cdh5.7.0</span><br><span class="line">sbin/start-dfs.sh <span class="comment"># 启动 hdfs</span></span><br><span class="line">jps <span class="comment"># 验证是否启动成功</span></span><br><span class="line">sbin/stop-dfs.sh <span class="comment"># 关闭 hdfs</span></span><br></pre></td></tr></table></figure></p><p>到浏览器查看，默认端口为 50070</p><p><img src="/images/hadoop/AE2C91C3-730B-433A-840C-8BFD2E9A53B2.png" alt="HDFS浏览器查看"></p><p><strong>修改 YARN 配置</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/hadoop-2.6.0-cdh5.7.0/etc/hadoop</span><br><span class="line">vim mapred-site.xml</span><br><span class="line">&lt;configuration&gt;                                                                                                                                         </span><br><span class="line">    &lt;property&gt;                                                                                                                                          </span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">vim yarn-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p><p><strong>启动 YARN</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/hadoop-2.6.0-cdh5.7.0</span><br><span class="line">sbin/start-yarn.sh <span class="comment"># 启动 hdfs</span></span><br><span class="line">jps <span class="comment"># 验证是否启动成功</span></span><br><span class="line">sbin/stop-yarn.sh <span class="comment"># 关闭 hdfs</span></span><br></pre></td></tr></table></figure></p><p>到浏览器查看，默认端口为 8088</p><p><img src="/images/hadoop/1E000CCD-4F73-4657-84D0-2D8929333486.png" alt="YARN浏览器查看"></p><p><strong>提交 MR 作业到 YARN 测试</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce</span><br><span class="line">hadoop jar hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar pi 2 3</span><br></pre></td></tr></table></figure></p><p><img src="/images/hadoop/806292DB-AFFA-4430-BCAE-39CF6EB755DA.png" alt="查看MR运行"></p><h3 id="分布式安装"><a href="#分布式安装" class="headerlink" title="分布式安装"></a>分布式安装</h3><h4 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h4><table><thead><tr><th>机器名</th><th>内网IP</th><th>节点</th></tr></thead><tbody><tr><td>instance-1</td><td>10.146.0.3</td><td>NameNode/DataNode  ResourceManager/NodeManager</td></tr><tr><td>instance-2</td><td>10.146.0.4</td><td>DataNode  NodeManager</td></tr><tr><td>instance-3</td><td>10.146.0.5</td><td>DataNode  NodeManager</td></tr></tbody></table><p>登录每台机器设置 hosts<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br><span class="line">10.146.0.3 instance-1</span><br><span class="line">10.146.0.4 instance-2</span><br><span class="line">10.146.0.5 instance-3</span><br></pre></td></tr></table></figure></p><h4 id="ssh-免密码登录"><a href="#ssh-免密码登录" class="headerlink" title="ssh 免密码登录"></a>ssh 免密码登录</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># instance-1，instance-2，instance-3</span></span><br><span class="line">yum install -y openssh-server</span><br><span class="line">service sshd start</span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line"><span class="comment"># instance-1</span></span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub instance-1</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub instance-2</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub instance-3</span><br></pre></td></tr></table></figure><h4 id="安装-JDK-1"><a href="#安装-JDK-1" class="headerlink" title="安装 JDK"></a>安装 JDK</h4><p>到 <a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">oracle 官网</a> 复制 JDK 下载地址。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># instance-1，instance-2，instance-3</span></span><br><span class="line">wget https://download.oracle.com/otn-pub/java/jdk/8u191-b12/2787e4a523244c269598db4e85c51e0c/jdk-8u191-linux-x64.tar.gz?AuthParam=1541057549_a8ee4101538c337d3b0c87d572832f2c</span><br><span class="line">tar -zxvf jdk-8u191-linux-x64.tar.gz\?AuthParam\=1541057549_a8ee4101538c337d3b0c87d572832f2c -C /usr/<span class="built_in">local</span>/</span><br><span class="line">vim /etc/profile</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_191</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib/tools.jar</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure></p><h4 id="安装-Hadoop-1"><a href="#安装-Hadoop-1" class="headerlink" title="安装 Hadoop"></a>安装 Hadoop</h4><p><strong>安装 hadoop-2.6.0-cdh5.7.0</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># instance-1，instance-2，instance-3</span></span><br><span class="line">wget http://archive-primary.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.0.tar.gz</span><br><span class="line">tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz -C /usr/<span class="built_in">local</span>/</span><br><span class="line">vim /etc/profile</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop-2.6.0-cdh5.7.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure></p><p><strong>修改 HDFS 配置</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># instance-1，instance-2，instance-3</span></span><br><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/hadoop-2.6.0-cdh5.7.0/etc/hadoop</span><br><span class="line">vim hadoop-env.sh <span class="comment"># 配置 JAVA_HOME</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_191</span><br><span class="line">vim core-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://instance-1:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">vim hdfs-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/usr/<span class="built_in">local</span>/hadoop-2.6.0-cdh5.7.0/dfs/name&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">         &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">         &lt;value&gt;/usr/<span class="built_in">local</span>/hadoop-2.6.0-cdh5.7.0/dfs/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">vim yarn-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;instance-1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">vim mapred-site.xml</span><br><span class="line">&lt;configuration&gt;                                                                                                                                         </span><br><span class="line">    &lt;property&gt;                                                                                                                                          </span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">vim slaves</span><br><span class="line">instance-1</span><br><span class="line">instance-2</span><br><span class="line">instance-3</span><br></pre></td></tr></table></figure></p><p><strong>格式化 NameNode</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># instance-1</span><br><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure></p><p><strong>启动集群</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/hadoop-2.6.0-cdh5.7.0</span><br><span class="line">sbin/start-all.sh <span class="comment"># 启动集群</span></span><br><span class="line">jps <span class="comment"># 验证是否启动成功</span></span><br><span class="line">sbin/stop-all.sh <span class="comment"># 关闭集群</span></span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> YARN </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
